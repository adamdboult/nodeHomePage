<h1 id="linear-forms">Linear forms</h1>
<h2 id="linear-forms-1">Linear forms</h2>
<p>A linear form is a linear map from a vector space to a scalar from the vector space’s underlying field.</p>
<p><span class="math inline">hom(<em>V</em>, <em>F</em>)</span></p>
<h3 id="matrix-operators">Matrix operators</h3>
<p>Linear forms can be represented as matrix operators.</p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>M</em> = <em>f</em></span></p>
<p>Where <span class="math inline"><em>M</em></span> has only one column.</p>
<h3 id="stuff">Stuff</h3>
<p><span class="math inline"><em>f</em>(<em>M</em>)=<em>f</em>(<em>v</em>)</span></p>
<p>We introduce <span class="math inline"><em>e</em><sub><em>i</em></sub></span>, the element vector. This is <span class="math inline">0</span> for all entries except for <span class="math inline"><em>i</em></span> where it is <span class="math inline">1</span>. Any vector can be shown as a sum of these vectors multiplied by a scalar.</p>
<p><span class="math inline">$f(M)=f(\sum^m_{i=1}a_{i}e_i)$</span></p>
<p><span class="math inline">$f(M)=\sum_{i=1}^mf(a_{i}e_i)$</span></p>
<p><span class="math inline">$f(M)=\sum_{i=1}^ma_if(e_i)$</span></p>
<p><span class="math inline">$f(M)=\sum_{i=1}^ma_if(e_i)$</span></p>
<h3 id="orthonormal-basis">Orthonormal basis</h3>
<p><span class="math inline">$f(M)=\sum_{i=1}^ma_i$</span></p>
<h2 id="dual-space">Dual space</h2>
<p>The dual space <span class="math inline"><em>V</em><sup>*</sup></span> of vector space <span class="math inline"><em>V</em></span> is the set of all linear forms, <span class="math inline">hom(<em>V</em>, <em>F</em>)</span>.</p>
<h3 id="the-dual-space-is-itself-a-vector-space">The dual space is itself a vector space</h3>
<p><span class="math inline"><em>v</em> ∈ <em>V</em></span></p>
<p><span class="math inline"><em>f</em> ∈ <em>F</em></span></p>
<p><span class="math inline"><em>a</em><em>v</em> = <em>f</em></span></p>
<p><span class="math inline"><em>b</em><em>v</em> = <em>g</em></span></p>
<p><span class="math inline">(<em>a</em> ⊕ <em>b</em>)<em>v</em> = <em>f</em> + <em>g</em></span></p>
<p><span class="math inline">(<em>a</em> ⊕ <em>b</em>)<em>v</em> = <em>a</em><em>v</em> + <em>b</em><em>v</em></span></p>
<p>So there is some operation we can do on two members of dual space</p>
<p>Linear in addition. That is, if we have two dual “things”, we can define the addition of functions as the operation which results int he outputs being added.</p>
<p>what about linear in scalar? same approach.</p>
<p>Well we define</p>
<p><span class="math inline"><em>c</em> ⊙ <em>a</em>)=<em>c</em><em>a</em><em>v</em></span></p>
<h3 id="the-dual-space-has-the-same-dimension-as-the-underlying-vector-space">The dual space has the same dimension as the underlying vector space</h3>
<h2 id="the-dual-space-forms-a-vector-space">The dual space forms a vector space</h2>
<p>The dual space forms a vector space. We can define addition and scalar multiplication on members of the dual space.</p>
<p>The dimenion of the dual space is the same as the underlying space.</p>
<p>We have defined the dual space. A vector in dual space will have also have components and a basis.</p>
<p><span class="math inline"><strong>w</strong> = ∑<sub><em>i</em></sub><em>w</em><sub><em>i</em></sub><em>f</em><sup><em>j</em></sup></span></p>
<p>So how we describe the components will depend on the choice of basis.</p>
<p>We choose the dual basis, the basis for <span class="math inline"><em>V</em><sup>*</sup></span> as:</p>
<p><span class="math inline"><strong>e</strong><sub><em>i</em></sub><strong>f</strong><sup><em>j</em></sup> = <em>δ</em><sub><em>i</em></sub><sup><em>j</em></sup></span></p>
<p>If the basis changes, so does the dual basis.</p>
<p>We write the dual basis as <span class="math inline"><em>e</em><sup><em>j</em></sup></span></p>
<h1 id="bilinear-forms">Bilinear forms</h1>
<h2 id="bilinear-forms-1">Bilinear forms</h2>
<p>A bilinear form takes two vectors and produces a scalar from the underyling field.</p>
<p>This is in contrast to a linear form, which only has one input.</p>
<p>In addition, the function is linear in both arguments.</p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>ϕ</em>(<em>a</em><em>u</em>, <em>b</em><em>v</em>)+<em>ϕ</em>(<em>a</em><em>u</em>, <em>y</em>)+<em>ϕ</em>(<em>x</em>, <em>b</em><em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>a</em><em>b</em><em>ϕ</em>(<em>u</em>, <em>v</em>)+<em>a</em><em>ϕ</em>(<em>u</em>, <em>y</em>)+<em>b</em><em>ϕ</em>(<em>x</em>, <em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<h3 id="representing-bilinear-forms">Representing bilinear forms</h3>
<p>They can be represented as:</p>
<p><span class="math inline"><em>ϕ</em>(<em>u</em>, <em>v</em>)=<em>v</em><sup><em>T</em></sup><em>M</em><em>u</em></span></p>
<p><span class="math inline"><em>f</em>(<em>M</em>)=<em>f</em>([<em>v</em><sub>1</sub>, <em>v</em><sub>2</sub>])</span></p>
<p>We introduce <span class="math inline"><em>e</em><sub><em>i</em></sub></span>, the element vector. This is <span class="math inline">0</span> for all entries except for <span class="math inline"><em>i</em></span> where it is <span class="math inline">1</span>. Any vector can be shown as a sum of these vectors multiplied by a scalar.</p>
<p><span class="math inline">$f(M)=f([\sum^m_{i=1}a_{1i}e_i,\sum^m_{i=1}a_{2i}e_i])$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^mf([a_{1k}e_k,\sum^m_{i=1}a_{2i}e_i])$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}f([a_{1k}e_k,a_{2i}e_i])$</span></p>
<p>Because this in linear in scalars:</p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}a_{2i}f([e_k,e_i])$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}a_{2i}e_k^TMe_i$</span></p>
<h3 id="orthonormal-basis-and-mi">Orthonormal basis and <span class="math inline"><em>M</em> = <em>I</em></span></h3>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}a_{2i}e_k^Te_i$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}a_{2i}\delta_i^k$</span></p>
<p><span class="math inline">$f(M)=\sum^m_{i=1}a_{1i}a_{2i}$</span></p>
<h2 id="the-dot-product">The dot product</h2>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>M</em><em>u</em> = <em>f</em></span></p>
<p>If the operator is <span class="math inline"><em>I</em></span> then we have the dot product.</p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>u</em></span></p>
<h2 id="orthogonal-vectors">Orthogonal vectors</h2>
<p>Given a metric <span class="math inline"><em>M</em></span>, two vectors <span class="math inline"><em>v</em></span> and <span class="math inline"><em>u</em></span> are orthogonal if:</p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>M</em><em>u</em> = 0</span></p>
<p>For example if we have the metric <span class="math inline"><em>M</em> = <em>I</em></span>, then two vectors are orthogonal if:</p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>u</em> = 0</span></p>
<h2 id="metric-preserving-transformations-and-isometry-groups">Metric-preserving transformations and isometry groups</h2>
<p>If we have a bilinear form we can write the form as:</p>
<p><span class="math inline"><em>u</em><sup><em>T</em></sup><em>M</em><em>v</em></span></p>
<p>After a transformation <span class="math inline"><em>P</em></span> to the vectors it is:</p>
<p><span class="math inline">(<em>P</em><em>u</em>)<sup><em>T</em></sup><em>M</em>(<em>P</em><em>v</em>)</span></p>
<p><span class="math inline"><em>u</em><sup><em>T</em></sup><em>P</em><sup><em>T</em></sup><em>M</em><em>P</em><em>v</em></span></p>
<p>So the value of the metric will be unaffected if:</p>
<p><span class="math inline"><em>u</em><sup><em>T</em></sup><em>P</em><sup><em>T</em></sup><em>M</em><em>P</em><em>v</em> = <em>u</em><sup><em>T</em></sup><em>M</em><em>v</em></span></p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>M</em><em>P</em> = <em>M</em></span></p>
<h3 id="equivalent-metrics">Equivalent metrics</h3>
<p>Different metrics can produce the same group. For example multiplying the metric by a constant.</p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>M</em><em>P</em> = <em>M</em></span></p>
<h2 id="orthogonal-groups-on-f">Orthogonal groups <span class="math inline"><em>O</em>(<em>n</em>, <em>F</em>)</span></h2>
<h3 id="recap-metric-preserving-transformations">Recap: Metric-preserving transformations</h3>
<p>The bilinear form is:</p>
<p><span class="math inline"><em>u</em><sup><em>T</em></sup><em>M</em><em>v</em></span></p>
<p>The transformations which preserve this are:</p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>M</em><em>P</em> = <em>M</em></span></p>
<h3 id="the-orthogonal-group">The orthogonal group</h3>
<p>If the metic is <span class="math inline"><em>M</em> = <em>I</em></span> then the condition is:</p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>P</em> = <em>I</em></span></p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup> = <em>P</em><sup>−1</sup></span></p>
<p>These form the orthogonal group.</p>
<p>We use <span class="math inline"><em>O</em></span> instead of <span class="math inline"><em>P</em></span>:</p>
<p><span class="math inline"><em>O</em><sup><em>T</em></sup> = <em>O</em><sup>−1</sup></span></p>
<h3 id="rotations-and-reflections">Rotations and reflections</h3>
<p>The orthogonal group is the rotations and reflections.</p>
<h3 id="parameters-of-the-orthogonal-group">Parameters of the orthogonal group</h3>
<p>The orthogonal group depends on the dimension of the vector space, and the underlying field. So we can have:</p>
<ul>
<li><p><span class="math inline"><em>O</em>(<em>n</em>, <em>R</em>)</span>; and</p></li>
<li><p><span class="math inline"><em>O</em>(<em>n</em>, <em>C</em>)</span>.</p></li>
</ul>
<h3 id="we-generally-refer-only-to-the-reals">We generally refer only to the reals</h3>
<p><span class="math inline"><em>O</em>(<em>n</em>)</span> means <span class="math inline"><em>O</em>(<em>n</em>, <em>R</em>)</span>.</p>
<p>The generally refer to the reals only.</p>
<h2 id="indefinite-pseduo-and-split-orthognal-groups-onmf">Indefinite (pseduo) and split orthognal groups <span class="math inline"><em>O</em>(<em>n</em>, <em>m</em>, <em>F</em>)</span></h2>
<h3 id="recap-metric-preserving-transformations-1">Recap: Metric-preserving transformations</h3>
<p>The bilinear form is:</p>
<p><span class="math inline"><em>u</em><sup><em>T</em></sup><em>M</em><em>v</em></span></p>
<p>The transformations which preserve this are:</p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>M</em><em>P</em> = <em>M</em></span></p>
<h3 id="the-metric">The metric</h3>
<p>If the metric is:</p>
<p><span class="math inline">$M=\begin{bmatrix}-1 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1 &amp; 1\\0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}$</span></p>
<p>Then we have the indefinite orthogonal group <span class="math inline"><em>O</em>(3, 1)</span></p>
<h3 id="the-split-orthogonal-group">The split orthogonal group</h3>
<p>Where <span class="math inline"><em>n</em> = <em>m</em></span> we have the split orthogonal group.</p>
<p><span class="math inline"><em>O</em>(<em>n</em>, <em>n</em>, <em>F</em>)</span></p>
<h3 id="signatures">Signatures</h3>
<h2 id="the-lorentz-group">The Lorentz group</h2>
<p>The Lorentz group is the <span class="math inline"><em>O</em>(1, 3)</span> group.</p>
<h3 id="symmetries-of-the-lorentz-group">Symmetries of the Lorentz group</h3>
<p>We can do the usual <span class="math inline">3</span> rotations, however there are additional <span class="math inline">3</span> symmetries, making the Lorzentz group <span class="math inline">6</span>-dimensional.</p>
<p>These are the Lorentz boosts.</p>
<p>A symmetry has:</p>
<p><span class="math inline"><em>t</em>′<sup>2</sup> − <em>x</em>′2 − <em>y</em>′<sup>2</sup> − <em>z</em>′<sup>2</sup> = <em>t</em><sup>2</sup> − <em>x</em><sup>2</sup> − <em>y</em><sup>2</sup> − <em>z</em><sup>2</sup></span></p>
<p>We consider the case wherewe just boost on <span class="math inline"><em>x</em></span>, so <span class="math inline"><em>y</em> = <em>y</em>′</span> and <span class="math inline"><em>z</em> = <em>z</em>′</span>.</p>
<p><span class="math inline"><em>t</em>′<sup>2</sup> − <em>x</em>′2 = <em>t</em><sup>2</sup> − <em>x</em><sup>2</sup></span></p>
<p>Or with <span class="math inline"><em>c</em></span>:</p>
<p><span class="math inline"><em>c</em><sup>2</sup><em>t</em>′<sup>2</sup> − <em>x</em>′2 = <em>t</em><sup>2</sup> − <em>x</em><sup>2</sup></span></p>
<h1 id="sesquilinear-forms">Sesquilinear forms</h1>
<h2 id="sesquilinear-forms-1">Sesquilinear forms</h2>
<h3 id="bilinear-form-recap">Bilinear form recap</h3>
<p>A bilinear form takes two vectors and produces a scalar from the underyling field.</p>
<p>The function is linear in addition in both arguments.</p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>ϕ</em>(<em>a</em><em>u</em>, <em>b</em><em>v</em>)+<em>ϕ</em>(<em>a</em><em>u</em>, <em>y</em>)+<em>ϕ</em>(<em>x</em>, <em>b</em><em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<p>The function is also linear in multiplication in both arguments.</p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>a</em><em>b</em><em>ϕ</em>(<em>u</em>, <em>v</em>)+<em>a</em><em>ϕ</em>(<em>u</em>, <em>y</em>)+<em>b</em><em>ϕ</em>(<em>x</em>, <em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<p>They can be represented as:</p>
<p><span class="math inline"><em>ϕ</em>(<em>u</em>, <em>v</em>)=<em>v</em><sup><em>T</em></sup><em>M</em><em>u</em></span></p>
<h3 id="sesquilinear-forms-2">Sesquilinear forms</h3>
<p>Like bilinear forms, sesquilinear are linear in addition:</p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>ϕ</em>(<em>a</em><em>u</em>, <em>b</em><em>v</em>)+<em>ϕ</em>(<em>a</em><em>u</em>, <em>y</em>)+<em>ϕ</em>(<em>x</em>, <em>b</em><em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<p>Sesqulinear forms however are only multiplictively linear in the second argument.</p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>b</em><em>ϕ</em>(<em>a</em><em>u</em>, <em>v</em>)+<em>ϕ</em>(<em>a</em><em>u</em>, <em>y</em>)+<em>b</em><em>ϕ</em>(<em>x</em>, <em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<p>In the first argument they are “twisted”</p>
<p><span class="math inline"><em>ϕ</em>(<em>a</em><em>u</em> + <em>x</em>, <em>b</em><em>v</em> + <em>y</em>)=<em>ā</em><em>b</em><em>ϕ</em>(<em>u</em>, <em>v</em>)+<em>ā</em><em>ϕ</em>(<em>u</em>, <em>y</em>)+<em>b</em><em>ϕ</em>(<em>x</em>, <em>v</em>)+<em>ϕ</em>(<em>x</em>, <em>y</em>)</span></p>
<h3 id="the-real-field">The real field</h3>
<p>For the real field, <span class="math inline"><em>b̄</em> = <em>b</em></span> and so the sesqulinear form is the same as the bilinear form.</p>
<h3 id="representing-sesquilinear-forms">Representing sesquilinear forms</h3>
<p>We can show the sesquilinear form as <span class="math inline"><em>v</em><sup>*</sup><em>M</em><em>u</em></span></p>
<h3 id="stuff-1">Stuff</h3>
<p><span class="math inline"><em>f</em>(<em>M</em>)=<em>f</em>([<em>v</em><sub>1</sub>, <em>v</em><sub>2</sub>])</span></p>
<p>We introduce <span class="math inline"><em>e</em><sub><em>i</em></sub></span>, the element vector. This is <span class="math inline">0</span> for all entries except for <span class="math inline"><em>i</em></span> where it is <span class="math inline">1</span>. Any vector can be shown as a sum of these vectors multiplied by a scalar.</p>
<p><span class="math inline">$f(M)=f([\sum^m_{i=1}a_{1i}e_i,\sum^m_{i=1}a_{2i}e_i])$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^mf([a_{1k}e_k,\sum^m_{i=1}a_{2i}e_i])$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}f([a_{1k}e_k,a_{2i}e_i])$</span></p>
<p>Because this in linear in scalars:</p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}^*a_{2i}f([e_k,e_i])$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}^*a_{2i}e_k^*Me_i$</span></p>
<h3 id="orthonormal-basis-and-mi-1">Orthonormal basis and <span class="math inline"><em>M</em> = <em>I</em></span></h3>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}^*a_{2i}e_k^*Me_i$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}^*a_{2i}e_k^*e_i$</span></p>
<p><span class="math inline">$f(M)=\sum_{k=1}^m\sum^m_{i=1}a_{1k}^*a_{2i}\delta_i^k$</span></p>
<p><span class="math inline">$f(M)=\sum^m_{i=1}a_{1i}^*a_{2i}$</span></p>
<h2 id="unitary-groups-un-f">Unitary groups <span class="math inline"><em>U</em>(<em>n</em>, <em>F</em>)</span></h2>
<h3 id="metric-preserving-transformations-for-sesquilinear-forms">Metric preserving transformations for sesquilinear forms</h3>
<p>For bilinear forms, the transformations which preserved metrics were:</p>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup> = <em>P</em><sup>−1</sup></span></p>
<p>For sesquilinear they are different:</p>
<p><span class="math inline"><em>u</em><sup>*</sup><em>M</em><em>v</em></span></p>
<p><span class="math inline">(<em>P</em><em>u</em>)<sup>*</sup><em>M</em>(<em>P</em><em>v</em>)</span></p>
<p><span class="math inline"><em>u</em><sup>*</sup><em>P</em><sup>*</sup><em>M</em><em>P</em><em>v</em></span></p>
<p>So we want the matrices where:</p>
<p><span class="math inline"><em>P</em><sup>*</sup><em>M</em><em>P</em> = <em>M</em></span></p>
<h3 id="the-unitary-group">The unitary group</h3>
<p>The unitary group is where <span class="math inline"><em>M</em> = <em>I</em></span></p>
<p><span class="math inline"><em>P</em><sup>*</sup><em>P</em> = <em>I</em></span></p>
<p><span class="math inline"><em>P</em><sup>*</sup> = <em>P</em><sup>−1</sup></span></p>
<p>We refer to these using <span class="math inline"><em>U</em></span> instead of <span class="math inline"><em>P</em></span>.</p>
<p><span class="math inline"><em>U</em><sup>*</sup> = <em>U</em><sup>−1</sup></span></p>
<h3 id="parameters-of-the-unitary-group">Parameters of the unitary group</h3>
<p>The unitary group depends on the dimension of the vector space, and the underlying field. So we can have:</p>
<ul>
<li><p><span class="math inline"><em>U</em>(<em>n</em>, <em>R</em>)</span>; and</p></li>
<li><p><span class="math inline"><em>U</em>(<em>n</em>, <em>C</em>)</span>.</p></li>
</ul>
<h3 id="we-generally-refer-only-to-the-complex">We generally refer only to the complex</h3>
<p>For the <span class="math inline"><em>U</em>(<em>n</em>, <em>R</em>)</span> we have:</p>
<p><span class="math inline"><em>U</em><sup>*</sup> = <em>U</em><sup>−1</sup></span></p>
<p><span class="math inline"><em>U</em><sup><em>T</em></sup> = <em>U</em><sup>−1</sup></span></p>
<p>This is the condition for the orthogonal group, and so we would instead write <span class="math inline"><em>O</em>(<em>n</em>)</span>.</p>
<p>As a result, <span class="math inline"><em>U</em>(<em>n</em>)</span> refers to <span class="math inline"><em>U</em>(<em>n</em>, <em>C</em>)</span>.</p>
<h3 id="u1-the-circle-group"><span class="math inline"><em>U</em>(1)</span>: The circle group</h3>
<h1 id="inner-products">Inner products</h1>
<h2 id="symmetric-matrices">Symmetric matrices</h2>
<h2 id="hermitian-self-adjoint-matrices">Hermitian (self-adjoint) matrices</h2>
<p>A matrix where <span class="math inline"><em>M</em> = <em>M</em><sup>*</sup></span></p>
<p>For matrices over the real numbers, these are the same as symmetric matrices.</p>
<h3 id="sesqulinear-forms-on-hermitian-matrices">Sesqulinear forms on Hermitian matrices</h3>
<p><span class="math inline"><em>ϕ</em>(<em>u</em>, <em>v</em>)=<em>u</em><sup>*</sup><em>M</em><em>v</em></span></p>
<p><span class="math inline">(<em>u</em><sup>*</sup><em>M</em><em>v</em>)<sup>*</sup> = <em>v</em><sup>*</sup><em>M</em><sup>*</sup><em>u</em> = <em>v</em><sup>*</sup><em>M</em><em>u</em></span></p>
<p><span class="math inline">$\phi (u,v)=\overline {\phi (v,u)}$</span></p>
<h3 id="the-forms-on-the-same-vector-are-always-real">The forms on the same vector are always real</h3>
<p><span class="math inline">(<em>v</em><sup>*</sup><em>M</em><em>v</em>)<sup>*</sup> = <em>v</em><sup>*</sup><em>M</em><sup>*</sup><em>v</em> = <em>v</em><sup>*</sup><em>M</em><em>v</em></span></p>
<p>So we have:</p>
<p><span class="math inline">(<em>v</em><sup>*</sup><em>M</em><em>v</em>)<sup>*</sup> = <em>v</em><sup>*</sup><em>M</em><em>v</em></span></p>
<p>Which is only satisfied for reals.</p>
<h3 id="if-a-and-b-are-hermitian">If <span class="math inline"><em>A</em></span> and <span class="math inline"><em>B</em></span> are Hermitian</h3>
<p>If <span class="math inline"><em>A</em></span> and <span class="math inline"><em>B</em></span> are Hermitian, <span class="math inline"><em>A</em><em>B</em></span> is Hermitian if and only if <span class="math inline"><em>A</em><em>B</em></span> commutes.</p>
<p><span class="math inline">(<em>A</em><em>B</em>)<sup>*</sup> = <em>B</em><sup>*</sup><em>A</em><sup>*</sup> = <em>B</em><em>A</em></span></p>
<p>If it commutes then</p>
<p><span class="math inline">(<em>A</em><em>B</em>)<sup>*</sup> = <em>A</em><em>B</em></span></p>
<h3 id="real-eigenvalues">Real eigenvalues</h3>
<p>Hermitian matrices have real eigenvalues.</p>
<p><span class="math inline"><em>H</em><em>v</em> = <em>λ</em><em>v</em></span></p>
<p><span class="math inline"><em>v</em><sup>*</sup><em>H</em><em>v</em> = <em>λ</em><em>v</em><sup>*</sup><em>v</em></span></p>
<p><span class="math inline"><em>v</em><sup>*</sup><em>H</em><em>v</em> = <em>λ</em></span></p>
<h3 id="skew-hermitian-matrices">Skew-Hermitian matrices</h3>
<p>These are also known as anti-Hermitian matrices.</p>
<p><span class="math inline"><em>M</em><sup>*</sup> = −<em>M</em></span></p>
<h3 id="if-eigenvalues-are-different-eigenvectors-are-orthogonal">If eigenvalues are different, eigenvectors are orthogonal</h3>
<h2 id="pauli-matrices">Pauli matrices</h2>
<p>Pauli matrices are <span class="math inline">2 × 2</span> matrices which are unitary and hermitian.</p>
<p>That is, <span class="math inline"><em>P</em><sup>*</sup> = <em>P</em><sup>−1</sup></span>.</p>
<p>And <span class="math inline"><em>P</em><sup>*</sup> = <em>P</em></span>.</p>
<h3 id="the-pauli-matrices">The Pauli matrices</h3>
<p>The matrices are:</p>
<p><span class="math inline">$\sigma_1 =\begin{bmatrix} 0&amp;1  \\ 1&amp;0  \end{bmatrix}$</span></p>
<p><span class="math inline">$\sigma_2 =\begin{bmatrix} 0&amp;-i \\ i&amp;0  \end{bmatrix}$</span></p>
<p><span class="math inline">$\sigma_3 =\begin{bmatrix} 1&amp;0  \\ 0&amp;-1 \end{bmatrix}$</span></p>
<p>The identity matrix is often considered alongside these as:</p>
<p><span class="math inline">$\sigma_0 =\begin{bmatrix} 1&amp;0  \\ 0&amp;1  \end{bmatrix}$</span></p>
<h3 id="pauli-matrices-are-their-own-inverse">Pauli matrices are their own inverse</h3>
<p><span class="math inline"><em>σ</em><sub><em>i</em></sub><sup>2</sup> = <em>σ</em><sub><em>i</em></sub><em>σ</em><sub><em>i</em></sub></span></p>
<p><span class="math inline"><em>σ</em><sub><em>i</em></sub><sup>2</sup> = <em>σ</em><sub><em>i</em></sub><em>σ</em><sub><em>i</em></sub><sup>*</sup></span></p>
<p><span class="math inline"><em>σ</em><sub><em>i</em></sub><sup>2</sup> = <em>σ</em><sub><em>i</em></sub><em>σ</em><sub><em>i</em></sub><sup>−1</sup></span></p>
<p><span class="math inline"><em>σ</em><sub><em>i</em></sub><sup>2</sup> = <em>I</em></span></p>
<h3 id="determinants-and-trace-of-pauli-matrices">Determinants and trace of Pauli matrices</h3>
<p><span class="math inline">det <em>σ</em><sub><em>i</em></sub> = −1</span></p>
<p><span class="math inline"><em>T</em><em>r</em>(<em>σ</em><sub><em>i</em></sub>)=0</span></p>
<p>As the sum of eigenvalues is the trace, and the product is the determinant, the eigenvalues are <span class="math inline">1</span> and <span class="math inline">−1</span>.</p>
<h2 id="positive-definite-matrices">Positive-definite matrices</h2>
<p>The matrix <span class="math inline"><em>M</em></span> is positive definite if for all non-zero vectors the scalar is positive.</p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>M</em><em>v</em></span></p>
<p>We know that the outcome is a scalar, so:</p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>M</em><em>v</em> = (<em>v</em><sup><em>T</em></sup><em>M</em><em>v</em>)<sup><em>T</em></sup></span></p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup><em>M</em><em>v</em> = <em>v</em><sup><em>T</em></sup><em>M</em><sup><em>T</em></sup><em>v</em></span></p>
<p><span class="math inline"><em>v</em><sup><em>T</em></sup>(<em>M</em> − <em>M</em><sup><em>T</em></sup>)<em>v</em> = 0</span></p>
<h2 id="inner-products-1">Inner products</h2>
<p>An inner product is a sesquilinear form with a positive-definite Hermitian matrix.</p>
<p><span class="math inline">⟨<em>u</em>, <em>v</em>⟩=<em>u</em><sup>*</sup><em>H</em><em>v</em></span></p>
<p>If we are using the real field this is the same as:</p>
<p><span class="math inline">⟨<em>u</em>, <em>v</em>⟩=<em>u</em><sup><em>T</em></sup><em>H</em><em>v</em></span></p>
<p>Where <span class="math inline"><em>H</em></span> is now a symmetric real matrix.</p>
<h3 id="same">Same</h3>
<p><span class="math inline">⟨<em>v</em>, <em>v</em>⟩=<em>v</em><sup>*</sup><em>H</em><em>v</em></span></p>
<p>Always positive and real.</p>
<h3 id="properties">Properties</h3>
<p><span class="math inline">⟨<em>u</em>, <em>v</em>⟩⟨<em>v</em>, <em>u</em>⟩=|⟨<em>u</em>, <em>v</em>⟩|<sup>2</sup></span></p>
<h2 id="cauchy-schwarz-inequality">Cauchy-Schwarz inequality</h2>
<p>This states that:</p>
<p><span class="math inline">$|\langle u,v\rangle |^2 \le \langle u, u\rangle \dot \langle v, v\rangle $</span></p>
<p>Consider the vectors <span class="math inline"><em>u</em></span> and <span class="math inline"><em>v</em></span>. We construct a third vector <span class="math inline"><em>u</em> − <em>λ</em><em>v</em></span>. We know the length of any vector is non-negative. <span class="math inline">0 ≤ ⟨<em>u</em> − <em>λ</em><em>v</em>, <em>u</em> − <em>λ</em><em>v</em>⟩</span></p>
<p><span class="math inline">0 ≤ ⟨<em>u</em>, <em>u</em>⟩+⟨<em>u</em>, −<em>λ</em><em>v</em>⟩+⟨−<em>λ</em><em>v</em>, <em>u</em>⟩+⟨−<em>λ</em><em>v</em>, −<em>λ</em><em>v</em>⟩</span></p>
<p><span class="math inline">0 ≤ ⟨<em>u</em>, <em>u</em>⟩−<em>λ̄</em>⟨<em>u</em>, <em>v</em>⟩−<em>λ</em>⟨<em>v</em>, <em>u</em>⟩ + <em>λ</em><em>λ̄</em>⟨<em>v</em>, <em>v</em>⟩</span></p>
<p>We now look for a value of <span class="math inline"><em>λ</em></span> to simplify this equation.</p>
<p><span class="math inline">$\lambda = \dfrac{\langle u,v \rangle}{\langle v, v\rangle}$</span></p>
<p><span class="math inline">$0\le \langle u, u\rangle-\dfrac{\langle v,u \rangle\langle u, v\rangle}{\langle v, v\rangle}-\dfrac{\langle u,v \rangle  \langle v, u\rangle }{\langle v, v\rangle}+ \dfrac{\langle u,v \rangle}{\langle v, v\rangle}\dfrac{\langle v,u \rangle}{\langle v, v\rangle}\langle v, v\rangle$</span></p>
<p><span class="math inline">$0\le \langle u, u\rangle-\dfrac{|\langle u,v \rangle|^2}{\langle v, v\rangle}$</span></p>
<p><span class="math inline">|⟨<em>u</em>, <em>v</em>⟩|<sup>2</sup> ≥ ⟨<em>u</em>, <em>u</em>⟩⟨<em>v</em>, <em>v</em>⟩</span></p>
<h1 id="multilinear-forms-and-determinants">Multilinear forms and determinants</h1>
<h2 id="multilinear-forms">Multilinear forms</h2>
<h2 id="determinants">Determinants</h2>
<p>From invertible matrix section in endo</p>
<p>A matrix can only be inverted if it can be created from a combination of elementary row operations.</p>
<p>How can we identify if a matrix is invertible? We want to create a scalar from the matrix which tells us if this possible. We can this scalar the determinant.</p>
<p>For a matrix <span class="math inline"><em>A</em></span> we label the determinant <span class="math inline">|<em>A</em>|</span>, or <span class="math inline">det <em>A</em></span></p>
<p>We propose <span class="math inline">|<em>A</em>|=0</span> when the matrix is not invertible.</p>
<p>So how can we identify the function we need to undertake on the matrix?</p>
<h3 id="new-1">New 1</h3>
<p>We know that linear dependence results in determinants of <span class="math inline">0</span>.</p>
<p>We can model this as a function on the columns of the matrix.</p>
<p><span class="math inline">det <em>M</em> = det([<em>M</em><sub>1</sub>, ..., <em>M</em><sub><em>n</em></sub>)</span></p>
<p>If there is linear depednence, for example if two columns are the same then:</p>
<p><span class="math inline">det([<em>M</em><sub>1</sub>, ..., <em>M</em><sub><em>i</em></sub>, ..., <em>M</em><sub><em>i</em></sub>, ..., <em>M</em><sub><em>n</em></sub>]) = 0</span></p>
<p>Similarly, if there is a column of <span class="math inline">0</span> then the determinant is <span class="math inline">0</span>.</p>
<p><span class="math inline">det([<em>M</em><sub>1</sub>, ..., 0, ..., <em>M</em><sub><em>n</em></sub>]) = 0</span></p>
<h3 id="new-2">New 2</h3>
<p>Show linear in addition</p>
<p>How can we identify the determinant of less simple matrices? We can use the multilinear form.</p>
<p><span class="math inline">∑<em>c</em><sub><em>i</em></sub><strong>M</strong><sub><em>i</em></sub> = <strong>0</strong></span></p>
<p>Where <span class="math inline"><strong>c</strong> ≠ <strong>0</strong></span></p>
<p>Or:</p>
<p><span class="math inline"><em>M</em><strong>c</strong> = <strong>0</strong></span></p>
<h3 id="rule-1-columns-of-matrices-can-be-the-input-to-a-multilinear-form">Rule 1: Columns of matrices can be the input to a multilinear form</h3>
<p>A matrix can be shown in terms of its columns. <span class="math inline"><em>A</em> = [<em>v</em><sub>1</sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">det <em>A</em> = det[<em>v</em><sub>1</sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">$\det A=\sum_{k_1=1}^m...\sum_{k_n=1}\prod_{i=1}^ma_{ik_i}\det ([e_{k_1},...,e_{k_n}])$</span></p>
<h3 id="multiplying-a-matrix-by-a-constant-multiplies-the-determinant-by-the-same-amount">Multiplying a matrix by a constant multiplies the determinant by the same amount</h3>
<p>If a whole row or columns is <span class="math inline">0</span> then:</p>
<p><span class="math inline">det <em>A</em> = det[<em>v</em><sub>1</sub>, ..., <em>v</em><sub><em>i</em></sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">det <em>A</em>′=det[<em>v</em><sub>1</sub>, ..., <em>c</em><em>v</em><sub><em>i</em></sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">det <em>A</em> = det[<em>v</em><sub>1</sub>, ..., <em>v</em><sub><em>i</em></sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">det <em>A</em>′=det[<em>v</em><sub>1</sub>, ..., <em>c</em><em>v</em><sub><em>i</em></sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">det <em>A</em>′=<em>c</em>det[<em>v</em><sub>1</sub>, ..., <em>v</em><sub><em>i</em></sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline">det <em>A</em>′=<em>c</em>det <em>A</em></span></p>
<p>As a result, multiplying a column by <span class="math inline">0</span> makes the determinant <span class="math inline">0</span>.</p>
<p>A matrix with a column of <span class="math inline">0</span> therefore has determinant <span class="math inline">0</span></p>
<h3 id="rule-2-a-matrix-with-equal-columns-has-a-determinant-of-0.">Rule 2: A matrix with equal columns has a determinant of <span class="math inline">0</span>.</h3>
<p><span class="math inline"><em>A</em> = [<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]</span></p>
<p><span class="math inline"><em>D</em>(<em>A</em>)=<em>D</em>([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>])</span></p>
<p>We know from Result 3 that swapping columns reverses the sign. Reversing columns results in the same matrix, so the determinant must be unchanged.</p>
<p><span class="math inline"><em>D</em>(<em>A</em>)= − <em>D</em>(<em>A</em>)</span></p>
<p><span class="math inline"><em>D</em>(<em>A</em>)=0</span></p>
<h3 id="linear-dependence">Linear dependence</h3>
<p>If a column is a linear combination of other columns, then the matrix cannot be inverted.</p>
<p><span class="math inline">$A=[a_1,...,\sum_{j\ne i}^{n}c_ja_j,...,a_n]$</span></p>
<p><span class="math inline">$\det A=\det ([v_1,...,\sum_{j\ne i}^{n}c_jv_j,...,v_n])$</span></p>
<p><span class="math inline">$\det A=\sum_{j\ne i}^{n}c_j\det ([v_1,...,v_j,...,v_n])$</span></p>
<p><span class="math inline">$\det A=\sum_{j\ne i}^{n}c_j\det ([v_1,...,v_j,,...,v_j,...,v_n])$</span></p>
<p>As there is a repeating vector:</p>
<p><span class="math inline">det <em>A</em> = 0</span></p>
<h3 id="swapping-columns-multiplies-the-determinant-by--1">Swapping columns multiplies the determinant by <span class="math inline">−1</span></h3>
<p><span class="math inline"><em>A</em> = [<em>v</em><sub>1</sub>, ..., <em>v</em><sub><em>i</em></sub> + <em>v</em><sub><em>j</em></sub>, ..., <em>v</em><sub><em>i</em></sub> + <em>v</em><sub><em>j</em></sub>, ..., <em>v</em><sub><em>n</em></sub>]</span></p>
<p>We know.</p>
<p><span class="math inline">det <em>A</em> = 0</span></p>
<p><span class="math inline">det <em>A</em> = det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>n</em></sub>])</span></p>
<p>So:</p>
<p><span class="math inline">det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) = 0</span></p>
<p>As <span class="math inline">2</span> of these have equal columns these are equal to <span class="math inline">0</span>.</p>
<p><span class="math inline">det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) + det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) = 0</span></p>
<p><span class="math inline">det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>n</em></sub>]) = −det([<em>a</em><sub>1</sub>, ..., <em>a</em><sub><em>j</em></sub>, ..., <em>a</em><sub><em>i</em></sub>, ..., <em>a</em><sub><em>n</em></sub>])</span></p>
<h3 id="calculating-the-determinant">Calculating the determinant</h3>
<p>We have</p>
<p><span class="math inline">$\det A=\sum_{k_1=1}^m...\sum_{k_n=1}\prod_{i=1}^ma_{ik_i}\det ([e_{k_1},...,e_{k_n}])$</span></p>
<p>So what is the value of the determinant here?</p>
<p>We know that the determinant of the identity matrix is <span class="math inline">1</span>.</p>
<p>We know that the determinant of a matrix with identical columns is <span class="math inline">0</span>.</p>
<p>We know that swapping columns multiplies the determinant by <span class="math inline">−1</span>.</p>
<p>Therefore the determinants where the values of <span class="math inline"><em>k</em></span> are not all unique are <span class="math inline">0</span>.</p>
<p>The determinants of the others are either <span class="math inline">−1</span> or <span class="math inline">1</span> depending on how many swaps are required to restore to the identity matrix.</p>
<p>This is also shown as the Leibni formula.</p>
<p><span class="math inline">$\det A = \sum_{\sigma \in S_n}sgn (\sigma )\prod_{i=1}^na_{i,\sigma_i}$</span></p>
<h2 id="properties-of-determinants">Properties of determinants</h2>
<h3 id="identity">Identity</h3>
<p><span class="math inline">det <em>I</em> = 1</span></p>
<h3 id="multiplication">Multiplication</h3>
<p><span class="math inline">det(<em>A</em><em>B</em>)=det <em>A</em>det <em>B</em></span></p>
<h3 id="inverse">Inverse</h3>
<p><span class="math inline">$\det (M^{-1})=\dfrac{1}{\det M}$</span></p>
<p>We know this because:</p>
<p><span class="math inline">det(<em>M</em><em>M</em><sup>−1</sup>)=det <em>I</em> = 1</span></p>
<p><span class="math inline">det <em>M</em>det <em>M</em><sup>−1</sup> = 1</span></p>
<p><span class="math inline">$\det (M^{-1})=\dfrac{1}{\det M}$</span></p>
<h3 id="complex-cojugate">Complex cojugate</h3>
<p><span class="math inline">$\det (M^*)=\overline {\det M}$</span></p>
<h3 id="transpose">Transpose</h3>
<p><span class="math inline">det(<em>M</em><sup><em>T</em></sup>)=det <em>M</em></span></p>
<h3 id="addition">Addition</h3>
<p><span class="math inline">det(<em>A</em> + <em>B</em>)=det <em>A</em> + det <em>B</em></span></p>
<h3 id="scalar-multiplication">Scalar multiplication</h3>
<p><span class="math inline">det <em>c</em><em>M</em> = <em>c</em><sup><em>n</em></sup>det <em>M</em></span></p>
<h3 id="determinants-and-eigenvalues">Determinants and eigenvalues</h3>
<p>The determinant is equal to the product of the eigenvalues.</p>
<h2 id="determinants-of-2x2-and-3x3-matrices">Determinants of 2x2 and 3x3 matrices</h2>
<h3 id="the-determinant-of-a-2x2-matrix">The determinant of a 2x2 matrix</h3>
<p><span class="math inline">$M=\begin{bmatrix}a &amp; b\\c &amp; d\end{bmatrix}$</span></p>
<p><span class="math inline">|<em>M</em>|=<em>a</em><em>d</em> − <em>b</em><em>c</em></span></p>
<h3 id="the-determinant-of-a-3x3-matrix">The determinant of a 3x3 matrix</h3>
<p><span class="math inline">$M=\begin{bmatrix}a &amp; b &amp; c\\d &amp; e &amp; f\\g &amp; h &amp; i\end{bmatrix}$</span></p>
<p><span class="math inline">|<em>M</em>|=<em>a</em><em>e</em><em>i</em> + <em>b</em><em>f</em><em>g</em> + <em>c</em><em>d</em><em>h</em> − <em>c</em><em>e</em><em>g</em> − <em>d</em><em>b</em><em>i</em> − <em>a</em><em>f</em><em>h</em></span></p>
<h1 id="special-groups">Special groups</h1>
<h2 id="special-orthogonal-groups-son-f">Special orthogonal groups <span class="math inline"><em>S</em><em>O</em>(<em>n</em>, <em>F</em>)</span></h2>
<p>The special orthogonal group, <span class="math inline"><em>S</em><em>O</em>(<em>n</em>, <em>F</em>)</span>, is the subgroup of the orthogonal group where <span class="math inline">|<em>M</em>|=1</span>.</p>
<p>As a result it includes only the rotation operators, not the flip operators.</p>
<p><span class="math inline"><em>S</em><em>O</em>(3)</span> is rotations in 3d space.</p>
<p><span class="math inline"><em>S</em><em>O</em>(2)</span> is rotations in 2d space.</p>
<h3 id="determinant-of-the-orthogonal-group">Determinant of the orthogonal group</h3>
<p>The orthogonal group has determinants of <span class="math inline">−1</span> or <span class="math inline">1</span>.</p>
<p><span class="math inline"><em>O</em><sup><em>T</em></sup> = <em>O</em><sup>−1</sup></span></p>
<p><span class="math inline">det(<em>O</em><sup><em>T</em></sup>)=det(<em>O</em><sup>−1</sup>)</span></p>
<p><span class="math inline">$\det O=\dfrac{1}{\det O}$</span></p>
<p><span class="math inline">det <em>O</em> = ±1</span></p>
<h2 id="special-unitary-groups-sun-f">Special unitary groups <span class="math inline"><em>S</em><em>U</em>(<em>n</em>, <em>F</em>)</span></h2>
<p>The special unitary group, <span class="math inline"><em>S</em><em>U</em>(<em>n</em>, <em>F</em>)</span>, is the subgroup of <span class="math inline"><em>U</em>(<em>n</em>, <em>F</em>)</span> where the determinants are <span class="math inline">1</span>.</p>
<p>That is, <span class="math inline">|<em>M</em>|=1</span></p>
<h3 id="the-determinant-of-unitary-matrices">The determinant of unitary matrices</h3>
<p>The determinant of the unitary matrices is:</p>
<p><span class="math inline">det <em>U</em><sup>*</sup> = det <em>U</em><sup>−1</sup></span></p>
<p><span class="math inline">$(\det U)^*=\dfrac{1}{\det U} $</span></p>
<p><span class="math inline">(det <em>U</em>)<sup>*</sup>det <em>U</em> = 1</span></p>
<p><span class="math inline">||det <em>U</em>|| = 1</span></p>
<h2 id="special-lnear-groups-sln-f">Special lnear groups <span class="math inline"><em>S</em><em>L</em>(<em>n</em>, <em>F</em>)</span></h2>
<p>The special linear group, <span class="math inline"><em>S</em><em>L</em>(<em>n</em>, <em>F</em>)</span>, is the subgroup of <span class="math inline"><em>G</em><em>L</em>(<em>n</em>, <em>F</em>)</span> where the determinants are <span class="math inline">1</span>.</p>
<p>That is, <span class="math inline">|<em>M</em>|=1</span></p>
<p>These are endomorphisms, not forms.</p>
<h1 id="sort">Sort</h1>
<h2 id="normal-matrices">Normal matrices</h2>
<p><span class="math inline"><em>M</em><sup>*</sup><em>M</em> = <em>M</em><em>M</em><sup>*</sup></span></p>
<p>All symmetrix matrices are normal</p>
<p>All hermetitian matrices (inc subset symmetric) are normal</p>
<p>Normal matrix never defective</p>
