
\subsection{Too many variables}

If there are more independent variables than samples then OLS will not work. There will be an infinite number of perfect fits.

For example if we regression genetic information on height with \(1000\) people, there will be too little data to fit using OLS.

This is due to colinearity.

We could also have too many variables through the use of derived variables. For example if we choose to use \(x\), \(x^2\), \(x^3\) etc.

\subsubsection{Optimal sparce regression}

Optimal is \(\lambda = \sigma 2\sqrt {{2\log (pn)}{n}}\)

Relies on knowing \(\sigma \), which we may not.

Instead we can use root LASSO.

Minimise the squareroot of the sum of squares loss (over n) , and use \(\lambda = \sqrt{2\log (pn)/n}\)

Doesn't have \(\sigma \)

Lasso biased, estimators \(0\) for many.

\subsubsection{Post-LASSO}

We can use LASSO for model selection, then use OLS on only those estimators.

