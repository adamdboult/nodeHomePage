
\subsection{Generalised Method of Moments (GMM)}

\subsubsection{Introduction}

We have a function on the output and a parameter:

\(g(y, \theta )\)

A moment condition is that the expectation of such a function is \(0\).

\(m(\theta )=E[g(y, \theta )]=0\)

To do GMM, we estimate this using:

\(\hat m(\theta )=\dfrac{1}{n}\sum_ig(y_i, \theta )\)

We define:

\(\Omega = E[g(y, \theta )g(y, \theta)^T]\)

\(G=E[\Delta_\theta g(y, \theta)]\)

And then minimise the norm:

\(||\hat m(\theta )||^2_W=\hat m(\theta )^TW\hat m(\theta )\)

Where \(W\) is a positive definite matrix for the norm.

\(\Omega ^{-1}\) is most efficient. But we don't know this. It depends on \(\theta \).

We can estimate it if IID:

\(\hat W(\hat \theta )= (\dfrac{1}{n}\sum_i g(y, \hat \theta)g(y, \hat \theta)^T)^{-1}\)

\subsubsection{Two-step feasible GMM}

Estimate using \(\mathbf W=\mathbf I\)

Consistent, but not efficient.

\subsubsection{Moment conditions}

OLS:

\(E[x(y-x\theta)]=0\)

WLS

\(E[x(y-x\theta)/\sigma^(x)]=0\)

IV

\(E[z(y-x\theta)]=0\)

MLE

\(E[\Delta_\theta \ln f(x, \theta)]=0\)

