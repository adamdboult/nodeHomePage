
\subsection{Consistency and efficiency of estimators}

\subsubsection{Consistency}

A statistic \(\hat \theta \) is a consistent estimator for \(\theta \) if its error tends to \(0\).

That is:

\(\hat \theta\rightarrow^p \theta \)

We can show that an estimator is consistent if we can write:

\(\hat \theta -\theta \) as a function of \(n\), causing it to tend to \(0\).

\subsubsection{Efficiency}

Efficiency measures the speed at which a consistent estimator tends towards the true value.

The speed of this convergence is the efficiency. could be fairly efficient plus biased too	p Measured as:

\(e(\hat \theta )=\dfrac{\dfrac{1}{I(\theta )}}{Var (\hat \theta )}\)

If an estimator as an efficiency of \(1\) and is unbiased, it is efficient.

\subsubsection{Relative efficiency}

We can measure the relative efficiency of two consistent estimators:

The relative efficiency is the variance of the first estimator, divided by the variance of the second.

\subsubsection{Root-n estimators}

An estimator is root-n consistent if it is consistent and its variance is:

\(O(\dfrac{1}{n})\)

\subsubsection{\(n^\delta \)-convergent}

A consistent estimator is \(n^\delta \)-consistent if its variance is:

\(O(\dfrac{1}{n^{2 \delta }})\)

