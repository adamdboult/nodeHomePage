
\subsection{Entropy}

\subsubsection{Introduction}

Entropy measures the expected amount of information produced by a source.

H(P(x))=E(I(P(x))

Entropy is similar to variance, is the sense that both measure uncertainty.

Entropy, however, has no references to specific values of \(x\). If all values were multiplied by 100, or if parts of the distribution were cut up and swapped, entropy would be unaffected.

For a probability function \(p(z)\), its entropy is :

\(H(p)=-\int p(z)\ln p(z)dz\).

This is a measure of the spread of a distribution.

Negative infinity means no uncertainty

For a multivariate gaussian H=d/2 ln(2\pi e|\Sigma)

