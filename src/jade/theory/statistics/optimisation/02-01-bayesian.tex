
\subsection{Bayesian optimisation}

\subsubsection{Introduction}

If we have sampled from the hyperparameter space we know something about the shape.

Can we use this to inform where we should next look?

The shape of the function is \(y=f(\mathbf x)\)

We have observations \(\mathbf X\) and \(\mathbf y\).

So what's our posterior, \(P(y|\mathbf X, \mathbf y)\)?

\subsubsection{Exploration and exploitation}

The can be a tradeoff between:

\begin{itemize}
\item Exploring - which gives us a better shape for \(y=f(x)\); and
\item Exploiting - which gives us a better estimate for the global optimum.
\end{itemize}

\subsubsection{The surrogate function}

We do not know \(y=f(x)\), but we model it as:

\(z(x)=y(x)+\epsilon\)

We can then maximise \(z\)

\subsubsection{Proposing new candidates}

We want an algorithm which maps from our history of observations to a new candidate.

There are different approaches:

\begin{itemize}
\item Probability of improvement - Choosing one with the highest chance of a more optimal value
\item Expected improvement - Choosing one with the biggest expected increase in the optimal value
\item Entropy search - choosing one which reduces uncertainty about the global maximum.
\end{itemize}

