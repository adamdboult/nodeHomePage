
\subsection{Perceptron (step function)}

\subsubsection{The function}

If the sum is above \(0\), \(a(z)=1\). Otherwise, \(a(z)=0\).

\subsubsection{The derivative}

This has a differential of \(0\) at all point except \(0\), where it is undefined.

\subsubsection{Notes}

This function is not smooth.

These is the activation function used in the perceptron.

Perceptron data needs to be linearly separable to train.

Even if linearly separable, doesn't necessarily get the best outcome?

\subsection{Perceptron}

Perceptron: one node neural network. is one or zero depednign if weightd inputs enough. therefore is classiication

If error, update weights

Only works if linearly separable. ie can draw linear line completely separting all inputs

Neural network has more layers

Works if data is linear

How to treat node inputs: raw, sigmoid, {0,1}

For all of these want the cost function have only one solution, like least squares doe. not guaranteed for all

For logistic, want to make it convex. loss = -log(f(x)) or -log(1-f(x)) depending on correct y. this is convex

How to create node inputs: sigmoid, binary cutoff
