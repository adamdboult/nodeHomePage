
\subsection{Overfitting}

Role of lambda: high makes impact of more variables lower => high bias

Low makes impacts of more variables strong => high variance

Can trade off using cut off. only make positive if above 0.7

How to use? difficult, as lambda within cost!

Can do similarly to d:

Run for a range of lambda (eg 0, 0.01, 0.02, 0.04, 0.08:~10), then pick from cross validation set

Low lambda always has low cost for training set, but not for cv set..

Regularisation: add to error term the size of the term. penalised large parameters

May not fit outside sample

High bias: eg house prices and size. linear would have high bias for out of scope sample (underfitting)

High variance: making polynomial passing through all data (overfitting)

Can reduce overfitting by reducing features either manaually or using models

OR regularisation: keep all features, but reduce magnitude of theta

\subsubsection{Regularisation}

Make cost function include size of theta^2 values

min 1/2m [sum (h(x)-y)^2 + 1000 theta3 ^2 + 1000 theta4 ^2]

or more broadly:

min 1/2m[sum ..... + lamba sum thetaj^2]

Tend to not include theta 0 as convention, no regularisation

Update for linear regression is

theta j = theta j -alpha{(1/m)* sum(h(x)-y)xj + (lambda/m thetaj)}

theta j = theta j (1- alpha lambda / m) -alpha {(1/m)*sum(h(x)-y)xj}

This is the same as before, but theta j updates from a smaller theta j each time

Normal equation needs a change

(X'X)^-1X'y=theta'')

Now is

(X'X+lambdaI)^-1X y  ')

although for theta 0, lambda zero, so indentiy matrix, but first element 0

REGULARISATION FOR REGULARISATION

add to end of J(theta):

+ lamda/2m .sum thetaj^21

	p update for theta j j>0:
	p is a as linear regression, but h(x) is a different function
