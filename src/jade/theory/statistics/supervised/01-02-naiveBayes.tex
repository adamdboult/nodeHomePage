
\subsection{The Naive Bayes classifier}

\subsubsection{Calculating the Naive Bayes estimator}

With the Naive Bayes assumption we have:

\(P(y|x_1,x_2,...,x_n)\propto P(x_1|y)P(x_2|y)...P(x_n|y)P(y)\)

We now choose \(y\) which maximises this.

This is easier to calculate, as there is less of a sample restriction.

This is used when evidence is also in classes, as the chance of any individual outcome on a continuous probability is \(0\).

\subsubsection{Estimating \(P(y)\)}

We can easily calculate \(P(y)\), by looking at the frequency across the sample.

\subsubsection{Estimating \(P(x_1|y)\)}

Normally, \(P(x_1|y)=\frac{n_c}{n_y}\), where:

\begin{itemize}
\item \(n_c\) is the number of instances where the evidence is \(c\) and the label is \(y\).
\item \(n_y\) is the number of instances where the label is \(y\).
\end{itemize}

\subsubsection{Regularising the Naive Bayes estimator}

To reduce the risk of specific probabilities being zero, we can adjust them, so that:

\(P(x_1|y)=\frac{n_c+mp}{n_y+m}\), where:

\begin{itemize}
\item \(p\) is the prior probability. If this is unknown, use \(\frac{1}{k}\), where \(k\) is the number of classes.
\item \(m\) is a parameter called the equivilant sample size.
\end{itemize}

