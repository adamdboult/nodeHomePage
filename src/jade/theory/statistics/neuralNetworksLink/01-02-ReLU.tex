
\subsection{Rectified Linear Unit (ReLU)}

\subsubsection{The function}

\(a(z)=\max (0,z)\)

\subsubsection{The derivative}

Its differential is \(1\) for values of \(z\) above \(0\), and \(0\) for values of \(z\) below \(0\).

The differential is undefined at \(z=0\), however this is unlikely to occur in practice.

\subsubsection{Notes}

The ReLU activation function induces sparcity.

