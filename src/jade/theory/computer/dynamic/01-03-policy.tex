
\subsection{Policies}

A policy maps the state onto the action

\(a_t=\pi (s_t)\)

The policy does not need to change over time, as discounting is constant. That is, if the policy should be different in future, it should be different now.

The policy affects the transition model, and so we have \(P_\pi \).

\subsubsection{Optimal policy}

There exists a policy that is better than any other policy, under any starting state.

There is no closed form solution to finding the optimal policy.

There are instead iterative methods.

