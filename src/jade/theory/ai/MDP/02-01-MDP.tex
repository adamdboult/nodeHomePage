\subsection{Markov Decision Processes (MDPs)}

\subsubsection{Introduction}

\subsubsection{Actions}

In a MDP the agent can choose an action in each state. The action affects the transition matrix.

This means that the rewards depends on the actions taken. We now have:

\begin{itemize}
\item \(S\) - the state space
\item \(s_1\) - the initial state
\item \(A\) - the action space
\item \(P\) - the transition model
\item \(R\) - the reward distribution
\end{itemize}

