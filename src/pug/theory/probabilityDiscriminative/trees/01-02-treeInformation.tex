
\subsection{Training decision trees with information gain}

We can train a decision tree by starting out with the most simple tree - all outcomes in same node.

We can then do a greedy search to identify which split on the node is best.

We can then iterate this process on future nodes.

\subsubsection{Training with information gain}

We split nodes to increase maximum entropy.

Entropy is:

\(E= -\sum_i^n p_{i=1}\log_2 p_i\)

Where we are summing across all nodes.

\subsubsection{Information gain}

The gain in entropy is the original entropy - weighted by size entropy of each branch

\subsubsection{Information gain ratio}

