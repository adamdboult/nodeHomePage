
\subsection{Introduction}

somewhere near quantisation: mixture of experts (MoE) compression
page on Mixture of Experts models generally?
+ confirm - are these specific to LLMs, or neural networks more generally? see to be more general?

LightGBM
	Light Gradient Boosting Machine

gram matrix to get kernel (stats stuff)
page on boosting. xgboost?
page on one-shot, zero-shot and few-shot learning.


\subsection{linear regression for inference}

ordinary linear regression for inference split out:
+ gauss markov

\subsection{estimating discriminative}

Point variable estimates for discriminative models:
+ split out

page on Multilevel regression with poststratification (MRP)

\subsection{supervised machine learning}

split out ensemble methods:
+ boosting and bagging

something on additive models around non-parametric regression

split out non-parametric regression. many of those should be in linear

\subsection{advanced inference}

xlearner
tlearner

\subsection{supervised linear regression}

ordinary least squares for prediction split out
+ frisch-waugh-lovell
+ bayesian linear regression to bayrsian bit above
+ least trimmed squares
+ linear models
+ impact of outliers (leverage and cook's distance)

page on feature engineering


