
\subsection{Information}

\subsubsection{Criteria}

Self information measures surprise of outcome. also called a surprisal.

When we observe an outcome we get information. We can develop a measure for how much information is associated with a specific measurement.

Rule 1: Information is always positive

Rule 2: If \(P(x)=1\), the the information for \(I(P(x))=0\).

Rule 3: If two events are independent, then their information is additive.

\begin{itemize}
\item \(P(C)=P(A)P(B)\)
\item \(I(P(C))=I(P(A)P(B))\)
\item \(I(P(A))+I(P(B))=I(P(A)P(B))\)
\end{itemize}

\subsubsection{Choice of function}

A function which satisifes this is \(I(P(A))=-\log(P(A))\)

Any base can be used. 2 is most common, information is in units of bit then.

