

\subsection{SORT}

basic creation and training of deep nn.
section on convolution. section on transformers. also for keras and tensorflow. and jax.

\subsection{From NumPy to PyTorch}

Basic functions are the same.

\begin{verbatim}
import numpy as np
import torch
a_np = np.array([[0,1],[2,3]])
b_np = np.array([[4,5],[6,7]])
a_torch = torch.tensor([[0,1],[2,3]])
b_torch = torch.tensor([[4,5],[6,7]])

a+b
a-b
a*b
a/b
a@b
\end{verbatim}

Tensors can also be defined using NumPy arrays

\begin{verbatim}
tensor = torch.from_numpy(a_np)
\end{verbatim}


Where @ is elementwise multiplication and @ is matrix multiplication.

As with NumPy we can define arrays like this.
\begin{verbatim}
torch.eye(n)
torch.ones(i, j, k, ...)
torch.full(i, j, k, ...)
torch.rand(i, j, k, ...)
torch.zeroes(i, j, k, ..)
\end{verbatim}

We can also define arrays of specific dtypes, as in NumPy.

\begin{verbatim}
a_torch = torch.tensor([[0,1],[2,3]], dtype =torch.float32)
\end{verbatim}

\subsection{Attaching CUDA cores to PyTorch tensors}

If tensors are on CUDA cores then the CUDA cores will be used rather than the CPU.

Can check if CUDA is available:
\begin{verbatim}
torch.cuda.is_available()
\end{verbatim}

Can move a tensor to the GPU.

\begin{verbatim}
tensor_cpu = torch.randn(3,3)
tensor_gpu = tensor_cpu.cuda()
# or tensor_gpu = tensor.cpu.to("cuda")
tensor_back_to_cpu = tensor_gpu.cpu()
# or tensor_back_to_cpu = tensor_gpu.to("cpu")
\end{verbatim}

The .cuda() function can take an integer if there are multiple GPUs available.

Whole models can also be moved to the GPU.
model.cuda()

We can be dynamic
\begin{verbatim}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tensor = torch.randn(3, 3).to(device)  # This will be on GPU if CUDA is available, otherwise CPU
model = SomeNeuralNetwork().to(device)  # Move model to the same device
\end{verbatim}

